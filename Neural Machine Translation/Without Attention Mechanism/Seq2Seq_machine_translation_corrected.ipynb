# %% codecell
!pip install jupyterthemes
!jt -t chesterish
# %% codecell

# %% codecell
from __future__ import print_function,division
from builtins import input,range
# %% codecell
import os,sys,numpy as np,pandas as pd,matplotlib.pyplot as plt
from keras.layers import Input,LSTM,Dense,Embedding
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.optimizers import Adam,SGD,RMSprop
from keras.utils import to_categorical
from keras.models import Model
import keras
import keras_applications
# %% codecell
BATCH_SIZE =128
EPOCHS = 100
LATENT_DIM = 256
NUM_SAMPLES = 10000
MAX_SEQUENCE_LEN = 100
MAX_NUM_WORDS = 20000
EMBEDDING_DIM = 100
# %% codecell
#SENTENCE IN ORIGINAL LANGUAGE
input_texts=[]

#SENTENCE IN TARGET LANGUAGE
target_texts=[]

#SENTENCE IN TARGET LANGUAGE OFFSET BY  ONE
target_texts_input=[]


# %% codecell
t=0
for line in open('hin.txt'):
    if t > NUM_SAMPLES:
        break
    if '\t' not in line:
        continue
    eng , hin , junk = line.split('\t')
    hineos = hin + ' <eos>'
    hinsos = '<sos> ' + hin

    input_texts.append(eng)
    target_texts_input.append(hinsos)
    target_texts.append(hineos)
print(input_texts[998])
print(target_texts[998])
print(target_texts_input[998])
assert '<sos>' in target_texts_input[1]
# %% codecell
#TOKENIZATION of input
tokenizer_inputs = Tokenizer(num_words=MAX_NUM_WORDS)
tokenizer_inputs.fit_on_texts(input_texts)
input_sequences = tokenizer_inputs.texts_to_sequences(input_texts)
word2idx_input = tokenizer_inputs.word_index
print(len(word2idx_input))
#max length input
max_len_input = max(len(s) for s in input_sequences)
print(max_len_input)


#TOKENIZATION OF OUTPUT (BOTH WITH SOS AND EOS TOKEN)
tokenizer_outputs = Tokenizer(num_words=MAX_NUM_WORDS,filters = '')
targets_combined = target_texts + target_texts_input
tokenizer_outputs.fit_on_texts(targets_combined)
target_sequences = tokenizer_outputs.texts_to_sequences(target_texts)
target_sequences_inputs = tokenizer_outputs.texts_to_sequences(target_texts_input)
word2idx_output = tokenizer_outputs.word_index
print(len(word2idx_output))
#print(word2idx_output['<sos>']) #######isme sos find nhi kr paa rha############
#max length target
max_len_output = max(len(s) for s in target_sequences)
for k,v in word2idx_output.items():
    if k == '<sos>':
        print("sos found at idx",v)
    if k == '<eos>':
        print("eos found at idx",v)
    else:
        continue
# %% codecell
#PADDING

#padding for encoder inputs
encoder_inputs_padded = pad_sequences(input_sequences, maxlen=max_len_input)
print("encoder input shape",encoder_inputs_padded.shape)
print("encoder_data[0]",encoder_inputs_padded[0])

#padding for decoder teacher forcing input
decoder_inputs_padded = pad_sequences(target_sequences_inputs, maxlen=max_len_output,padding = 'post')
print("decoder input shape" , decoder_inputs_padded.shape)
print("decoder_data[0]" , decoder_inputs_padded)

#padding for decoder target
decoder_target_padded = pad_sequences(target_sequences,maxlen=max_len_output , padding = 'post')
print("decoder target  shape" , decoder_target_padded.shape)
print("decoder_target_data[0]" , decoder_target_padded[0])
# %% codecell
#LOADING PRETRAINED WORD VECTORS
print("loading vectors broo")
word2vec = {}
with open(os.path.join('glove.6B.100d.txt')) as f:
    for line in f:
        values = line.split()
        word = values[0]
        vec = np.asarray(values[1:],dtype='float32')
        word2vec[word] = vec
print("length of word2vec" , len(word2vec))
# %% codecell
#EMBEDDING MATRIX
print("filling pretrained matrix...hold on a sec....Done!")
num_words = min(MAX_NUM_WORDS,len(word2idx_input)+1)
embedding_matrix = np.zeros((num_words,EMBEDDING_DIM))
for word , i in word2idx_input.items():
    if i < MAX_NUM_WORDS:
        embedding_vector = word2vec.get(word)
        if embedding_vector is not None:
            embedding_matrix[i] = embedding_vector
# %% codecell
#EMBEDDING LAYER
embedding_layer = Embedding(num_words,EMBEDDING_DIM,weights = [embedding_matrix],input_length=max_len_input,trainable = True)
# %% codecell
num_words_output = len(word2idx_output)+1
# %% codecell
#CREATING ONE HOT TARGETS since we cant use sparse_categorical_crossentropy for sequence data
decoder_targets_one_hot = np.zeros((len(input_texts),max_len_output,num_words_output),dtype = 'float32')
for i , d in enumerate(decoder_target_padded):
    for t,word in enumerate(d):
        decoder_targets_one_hot[i,t,word] = 1
# %% codecell
#BUILD THE MODEL (TRAINING)

#ENCODER MODEL FOR TRAINING
encoder_inputs_placeholder = Input(shape = (max_len_input,))
x = embedding_layer(encoder_inputs_placeholder)
encoder =  LSTM(LATENT_DIM,return_state=True,return_sequences=False,recurrent_dropout=0.5)
encoder_outputs , h , c = encoder(x)
encoder_states = [h,c]


#DECODER MODEL FOR TEACHER FORCING TRAINING
decoder_inputs_placeholder = Input(shape = (max_len_output,))
#the embedding will not use the pretrained embeddings, so creating the new embedding layer
decoder_embedding = Embedding(num_words_output,EMBEDDING_DIM)
decoder_inputs_x = decoder_embedding(decoder_inputs_placeholder)
decoder_lstm = LSTM(LATENT_DIM,return_sequences=True,return_state=True,recurrent_dropout=0.5)
decoder_outputs, _,_= decoder_lstm(decoder_inputs_x,initial_state = encoder_states)
decoder_dense = Dense(num_words_output , activation = 'softmax')
decoder_outputs = decoder_dense(decoder_outputs)

model = Model([encoder_inputs_placeholder,decoder_inputs_placeholder],decoder_outputs)
model.compile(optimizer = Adam(lr = 0.001),loss = 'categorical_crossentropy',metrics = ['accuracy'])
model.summary()
# %% codecell
r = model.fit([encoder_inputs_padded,decoder_inputs_padded],decoder_targets_one_hot,batch_size=BATCH_SIZE,
             epochs=EPOCHS,validation_split=0.2)
# %% codecell
plt.plot(r.history['loss'],label = 'training_loss')
plt.plot(r.history['val_loss'],label = 'validation_loss')
plt.legend()
plt.show()

plt.plot(r.history['accuracy'],label = 'training_loss')
plt.plot(r.history['val_accuracy'],label = 'validation_accuracy')
plt.legend()
plt.show()
# %% markdown
# NOW WE WILL USE THE PREVIOUSLY TRAINED ENCODER MODEL AND CREATE A DECODER MODEL THAT GENERATES TEXT AFTER TAKING ENCODER INITIAL STATES AND ACCEPTING SEQUENCE LENGTH OF 1
# %% codecell
#LOADING ENCODER MODEL
encoder_model = Model(encoder_inputs_placeholder,encoder_states)

# %% codecell
decoder_states_input_h = Input(shape = (LATENT_DIM,))
decoder_states_input_c = Input(shape = (LATENT_DIM,))
decoder_states_inputs = [decoder_states_input_h ,decoder_states_input_c]
decoder_inputs_single = Input(shape = (1,))
decoder_inputs_single_x = decoder_embedding(decoder_inputs_single)
decoder_outputs,h,c = decoder_lstm(decoder_inputs_single_x,initial_state=decoder_states_inputs)
print("h",h.shape)
print("c",c.shape)
print("decoder_inputs_single", decoder_inputs_single)
decoder_states = [h,c]
decoder_outputs = decoder_dense(decoder_outputs)

#INITIALIZING THE DECODER MODEL
decoder_model = Model([decoder_inputs_single]+decoder_states_inputs,
                     [decoder_outputs]+decoder_states)
# %% codecell
#REVERSE IDX TO WORD MAPPING
idx2word_eng = {v:k for k,v in word2idx_input.items()}
idx2word_tran = {v:k for k,v in word2idx_output.items()}

# %% codecell
#SAMPLING FUNCTION
def decode_sequence(input_seq):
    states_value = encoder_model.predict(input_seq)
    #create empty decoder input
    target_seq = np.zeros((1,1))
    #fill the first pos of decoder input with sos token as we will input sos and encoder states in decoder
    target_seq[0,0] = word2idx_output['<sos>']
    eos = word2idx_output['<eos>']
    #creating the translation
    output_sentence = []
    for _ in range(max_len_output):
        out,h,c = decoder_model.predict([target_seq]+states_value)
        #extract word with highest probability
        idx = np.argmax(out[0,0,:])
        if eos == idx:
            break
        word = ''
        if idx>0:
            word = idx2word_tran[idx]
            output_sentence.append(word)
        #update the decoder input for the next cell
        target_seq[0,0] = idx
        #update the states too
        states_value = [h,c]
    return ' '.join(output_sentence)
# %% codecell
#TEST TRANSLATIONS::
#while True:
    i = np.random.choice(len(input_texts))
    input_seq = encoder_inputs_padded[i:i+1]
    print(input_seq)
    print("shape",input_seq.shape)
    translation = decode_sequence(input_seq)
    print('_')
    print("input",input_texts[i])
    print("translation" , translation)

    ans = input("continue y/n ?")
    if ans and ans.lower().startswith('n'):
        break
# %% codecell
#SAMPLING FUNCTION #FOR CUSTOM INPUT
def decode_sequence(input_seq):
    states_value = encoder_model.predict(input_seq)
    #create empty decoder input
    target_seq = np.zeros((1,1))
    #fill the first pos of decoder input with sos token as we will input sos and encoder states in decoder
    target_seq[0,0] = word2idx_output['<sos>']
    eos = word2idx_output['<eos>']
    #creating the translation
    output_sentence = []
    for _ in range(max_len_output):
        out,h,c = decoder_model.predict([target_seq]+states_value)
        #extract word with highest probability
        idx = np.argmax(out[0,0,:])
        if eos == idx:
            break
        word = ''
        if idx>0:
            word = idx2word_tran[idx]
            output_sentence.append(word)
        #update the decoder input for the next cell
        target_seq[0,0] = idx
        #update the states too
        states_value = [h,c]
    return ' '.join(output_sentence)
# %% codecell
#FOR CUSTOM INPUT   ######NOT WORKING######
while True:
    i = input()
    tokenizer_inputs_custom =tokenizer_inputs.texts_to_sequences(i)
    decoder_inputs_padded_custom = pad_sequences(tokenizer_inputs_custom, maxlen=max_len_input)
    #print(input_sequences_custom)
    #print("shape of tokens",tokenizer_inputs_custom.shape)
    print("shape of padded custom input",decoder_inputs_padded_custom)
    translation = decode_sequence(decoder_inputs_padded_custom)
    print('_')
    print("input",i)
    print("translation" , translation)

    ans = input("continue y/n ?")
    if ans and ans.lower().startswith('n'):
        break
# %% codecell
